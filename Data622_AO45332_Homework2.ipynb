{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3MAAKjEU3RB",
        "outputId": "90c5473c-8874-4e35-a879-ee8ee7097c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- First 700 Characters (Raw HTML Content) ---\n",
            "\n",
            "  <!DOCTYPE html>\n",
            "<html lang=\"en\" data-uri=\"cms.cnn.com/_pages/cmboyzvxs00d626qmalw1heqy@published\" data-layout-uri=\"cms.cnn.com/_layouts/layout-with-rail/instances/style-article-feature-v1@published\" >\n",
            "  <head>\n",
            "<link rel=\"dns-prefetch\" href=\"//tpc.googlesyndication.com\">\n",
            "\n",
            "<link rel=\"preconnect\" href=\"//tpc.googlesyndication.com\">\n",
            "\n",
            "<link rel=\"dns-prefetch\" href=\"//pagead2.googlesyndication.com\">\n",
            "\n",
            "<link rel=\"preconnect\" href=\"//pagead2.googlesyndication.com\">\n",
            "\n",
            "<link rel=\"dns-prefetch\" href=\"//www.googletagservices.com\">\n",
            "\n",
            "<link rel=\"preconnect\" href=\"//www.googletagservices.com\">\n",
            "\n",
            "<link rel=\"dns-prefetch\" href=\"//www.google.com\">\n",
            "\n",
            "<link rel=\"preconnect\" href=\"//www.google.com\">\n",
            "\n",
            "<link rel=\"dns\n",
            "\n",
            "--- First 50 Lemmatized Words ---\n",
            "\n",
            "['luxury', 'brand', 'expensive', 'ever', 'telling', 'worth', 'cnn', 'cnn', 'value', 'feedback', 'relevant', 'ad', 'encounter', 'technical', 'issue', 'video', 'player', 'slow', 'load', 'content', 'video', 'content', 'never', 'loaded', 'ad', 'froze', 'finish', 'loading', 'video', 'content', 'start', 'ad', 'audio', 'ad', 'loud', 'issue', 'ad', 'never', 'loaded', 'ad', 'preventedslowed', 'page', 'loading', 'content', 'moved', 'around', 'ad', 'loaded', 'ad', 'repetitive']\n",
            "\n",
            "--- First 50 Stemmed Words ---\n",
            "\n",
            "['luxuri', 'brand', 'expens', 'ever', 'tell', 'worth', 'cnn', 'cnn', 'valu', 'feedback', 'relev', 'ad', 'encount', 'technic', 'issu', 'video', 'player', 'slow', 'load', 'content', 'video', 'content', 'never', 'load', 'ad', 'froze', 'finish', 'load', 'video', 'content', 'start', 'ad', 'audio', 'ad', 'loud', 'issu', 'ad', 'never', 'load', 'ad', 'preventedslow', 'page', 'load', 'content', 'move', 'around', 'ad', 'load', 'ad', 'repetit']\n",
            "\n",
            "--- Difference Observation ---\n",
            "Lemmatization keeps words meaningful (e.g., 'better' -> 'good'),\n",
            "while stemming often truncates words mechanically (e.g., 'expensive' -> 'expens').\n"
          ]
        }
      ],
      "source": [
        "# DATA 622 - Homework 2\n",
        "# Custom implementation with structured modular workflow\n",
        "\n",
        "import requests\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure required NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Added to address the LookupError\n",
        "\n",
        "def fetch_article_text(url):\n",
        "    \"\"\"Download article content and return first 700 characters (raw).\"\"\"\n",
        "    response = requests.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    return response.text[:700], response.text\n",
        "\n",
        "def strip_html(content):\n",
        "    \"\"\"Remove HTML tags and return clean text.\"\"\"\n",
        "    parser = BeautifulSoup(content, \"html.parser\")\n",
        "    return parser.get_text(separator=\" \")\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Lowercase text and remove punctuation.\"\"\"\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    \"\"\"Remove English stopwords.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words and word.isalpha()]\n",
        "\n",
        "def lemmatize_words(tokens):\n",
        "    \"\"\"Apply WordNet lemmatization.\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "def stem_words(tokens):\n",
        "    \"\"\"Apply Porter stemming.\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "def main():\n",
        "    article_url = \"https://www.cnn.com/2025/06/13/style/why-luxury-brands-are-so-expensive\"\n",
        "\n",
        "    # Step 1: Read file and print first 700 characters\n",
        "    raw_preview, full_html = fetch_article_text(article_url)\n",
        "    print(\"\\n--- First 700 Characters (Raw HTML Content) ---\\n\")\n",
        "    print(raw_preview)\n",
        "\n",
        "    # Step 2: Remove HTML tags\n",
        "    clean_text = strip_html(full_html)\n",
        "\n",
        "    # Step 3: Lowercase + remove punctuation\n",
        "    normalized_text = normalize_text(clean_text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(normalized_text)\n",
        "\n",
        "    # Step 4: Remove stopwords\n",
        "    filtered_tokens = remove_stop_words(tokens)\n",
        "\n",
        "    # Step 5: Lemmatization\n",
        "    lemmatized = lemmatize_words(filtered_tokens)\n",
        "    print(\"\\n--- First 50 Lemmatized Words ---\\n\")\n",
        "    print(lemmatized[:50])\n",
        "\n",
        "    # Comparison with stemming\n",
        "    stemmed = stem_words(filtered_tokens)\n",
        "    print(\"\\n--- First 50 Stemmed Words ---\\n\")\n",
        "    print(stemmed[:50])\n",
        "\n",
        "    print(\"\\n--- Difference Observation ---\")\n",
        "    print(\"Lemmatization keeps words meaningful (e.g., 'better' -> 'good'),\")\n",
        "    print(\"while stemming often truncates words mechanically (e.g., 'expensive' -> 'expens').\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}